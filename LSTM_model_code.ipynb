{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "###############################################################################\n",
    "# 1. Data preparation\n",
    "###############################################################################\n",
    "def create_sequences(data, window_size):\n",
    "    \"\"\"\n",
    "    Convert a multivariate sequence into samples of shape (window_size, n_features).\n",
    "    The target is the last column (log_beta).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i : i + window_size, :-1])  # use first 6 features as input\n",
    "        y.append(data[i + window_size, -1])         # predict normalized log_beta\n",
    "    \n",
    "    #X_df = pd.DataFrame(X[0], columns=['day', 'S', 'E', 'I', 'R', 'prev_I'])\n",
    "        # add y[0] as last column\n",
    "    #X_df['log_beta_target'] = y[0]\n",
    "        # save as CSV\n",
    "    #X_df.to_csv('first_sequence.csv', index=False)\n",
    "    #print(\"X[0] and y[0] are saved in first_sequence.csv\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_prepare_data(data_dir, num_seeds, window_size=7, test_seed=None):\n",
    "    \"\"\"\n",
    "    Loads CSV files, applies shift(-2) for 'prev_I', computes log(Beta) for 'log_beta',\n",
    "    and fits a global scaler on all 7 columns.\n",
    "    Returns training sequences, targets, and the scaler.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # First pass: collect data for scaling\n",
    "    for i in range(num_seeds):\n",
    "        if test_seed is not None and i == test_seed:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_dir, f'seir_seed_{i}.csv'))\n",
    "            df[['S', 'E', 'I', 'R', 'Beta']] = df[['S', 'E', 'I', 'R', 'Beta']].fillna(0)\n",
    "            if 'day' not in df.columns:\n",
    "                df['day'] = range(len(df))\n",
    "            df['prev_I'] = df['I'].shift(-2).fillna(0)\n",
    "            df['log_beta'] = np.log(df['Beta'].clip(lower=1e-7))\n",
    "            features = df[['day', 'S', 'E', 'I', 'R', 'prev_I', 'log_beta']].values\n",
    "            all_data.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping seed {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    if all_data:\n",
    "        scaler.fit(np.concatenate(all_data))\n",
    "        \n",
    "    # Second pass: create training sequences\n",
    "    all_X, all_y = [], []\n",
    "    for i in range(num_seeds):\n",
    "        if test_seed is not None and i == test_seed:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_dir, f'seir_seed_{i}.csv'))\n",
    "            df[['S', 'E', 'I', 'R', 'Beta']] = df[['S', 'E', 'I', 'R', 'Beta']].fillna(0)\n",
    "            if 'day' not in df.columns:\n",
    "                df['day'] = range(len(df))\n",
    "            df['prev_I'] = df['I'].shift(-2).fillna(0)\n",
    "            df['log_beta'] = np.log(df['Beta'].clip(lower=1e-7))\n",
    "            features = df[['day', 'S', 'E', 'I', 'R', 'prev_I', 'log_beta']].values\n",
    "            scaled_features = scaler.transform(features)\n",
    "            X, y = create_sequences(scaled_features, window_size)\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping seed {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return np.concatenate(all_X), np.concatenate(all_y), scaler\n",
    "\n",
    "###############################################################################\n",
    "# 2. Simplified model architecture with Grid search hyperparameters\n",
    "###############################################################################\n",
    "def build_lstm_model(window_size, n_features,\n",
    "                     lstm_units1=64, lstm_units2=64,\n",
    "                     dropout_rate1=0.2, dropout_rate2=0.3,\n",
    "                     optimizer='rmsprop'):\n",
    "    \"\"\"\n",
    "    Builds an LSTM model using the grid search best hyperparameters:\n",
    "      - First LSTM layer units: 64 with recurrent dropout of 0.2\n",
    "      - Second LSTM layer units: 64 with dropout of 0.3\n",
    "      - Optimizer: rmsprop\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(window_size, n_features))\n",
    "    x = LSTM(lstm_units1, return_sequences=True, recurrent_dropout=dropout_rate1)(inputs)\n",
    "    x = Dropout(dropout_rate1)(x)\n",
    "    x = LSTM(lstm_units2, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 3. Enhanced training configuration\n",
    "###############################################################################\n",
    "def lr_scheduler(epoch):\n",
    "    \"\"\"\n",
    "    Applies learning rate decay after 20 epochs.\n",
    "    \"\"\"\n",
    "    initial_lr = 0.001\n",
    "    decay = 0.1 if epoch > 20 else 1.0\n",
    "    return initial_lr * decay\n",
    "\n",
    "def train_lstm_model(X_train, y_train, window_size=7):\n",
    "    \"\"\"\n",
    "    Builds and trains the LSTM model with early stopping and a learning rate scheduler.\n",
    "    \"\"\"\n",
    "    model = build_lstm_model(window_size, X_train.shape[2])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, LearningRateScheduler(lr_scheduler)],\n",
    "        verbose=1\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "###############################################################################\n",
    "# 4. Simulation components\n",
    "###############################################################################\n",
    "class LSTMPredictor:\n",
    "    \"\"\"\n",
    "    Wraps the trained LSTM model to predict beta on a rolling window of\n",
    "    [day, S, E, I, R, prev_I] (6 features). \n",
    "    The model was trained to predict normalized log_beta, so this class\n",
    "    denormalizes the prediction and returns beta.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, full_scaler, window_size=7):\n",
    "        self.model = model\n",
    "        # Create a scaler for input features (first 6 columns)\n",
    "        self.input_scaler = StandardScaler()\n",
    "        self.input_scaler.mean_ = full_scaler.mean_[:6]\n",
    "        self.input_scaler.scale_ = full_scaler.scale_[:6]\n",
    "        self.input_scaler.var_ = full_scaler.var_[:6]\n",
    "        self.input_scaler.n_features_in_ = 6\n",
    "        self.window_size = window_size\n",
    "        self.buffer = []\n",
    "        # Store target parameters for log_beta (7th column)\n",
    "        self.target_mean = full_scaler.mean_[-1]\n",
    "        self.target_scale = full_scaler.scale_[-1]\n",
    "        \n",
    "    def update_buffer(self, new_data):\n",
    "        # new_data should be a list with 6 elements: [day, S, E, I, R, prev_I]\n",
    "        self.buffer.append(new_data)\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "            \n",
    "    def predict_next(self):\n",
    "        # Ensure the buffer has window_size rows\n",
    "        if len(self.buffer) < self.window_size:\n",
    "            padded = np.zeros((self.window_size, 6))\n",
    "            padded[-len(self.buffer):] = self.buffer\n",
    "        else:\n",
    "            padded = np.array(self.buffer[-self.window_size:])\n",
    "            \n",
    "        scaled = self.input_scaler.transform(padded)\n",
    "        scaled_window = scaled.reshape(1, self.window_size, 6)\n",
    "        normalized_pred = self.model.predict(scaled_window, verbose=0)[0][0]\n",
    "        # Denormalize to obtain the raw log_beta\n",
    "        raw_log_beta = normalized_pred * self.target_scale + self.target_mean\n",
    "        # Compute beta by exponentiating the log_beta\n",
    "        predicted_beta = np.exp(raw_log_beta)\n",
    "        return predicted_beta\n",
    "\n",
    "def simulate_seir_lstm(seed_data, start_day, predictor, max_days=200, stop_early=True):\n",
    "    \"\"\"\n",
    "    Simulates the SEIR model from start_day onward using predicted beta.\n",
    "    \"\"\"\n",
    "    # Initial conditions at start_day\n",
    "    initial = seed_data.iloc[start_day]\n",
    "    S, E, I, R, Beta = initial[['S', 'E', 'I', 'R', 'Beta']]\n",
    "    \n",
    "    gamma = 0.08\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Initialize predictor buffer using the last 'window_size' days\n",
    "    for i in range(max(0, start_day - predictor.window_size + 1), start_day + 1):\n",
    "        row = seed_data.iloc[i]\n",
    "        raw_features = [row['day'], row['S'], row['E'], row['I'], row['R'], row['prev_I']]\n",
    "        predictor.update_buffer(raw_features)\n",
    "    \n",
    "    history = {\n",
    "        'days': [start_day],\n",
    "        'S': [S],\n",
    "        'E': [E],\n",
    "        'I': [I],\n",
    "        'R': [R],\n",
    "        'Beta': [Beta]\n",
    "    }\n",
    "    \n",
    "    for day in range(1, max_days + 1):\n",
    "        current_day = start_day + day\n",
    "        \n",
    "        # Predict beta (this prediction already inverts the normalization)\n",
    "        predicted_beta = predictor.predict_next()\n",
    "        \n",
    "        # SEIR update equations\n",
    "        new_exposed = predicted_beta * S * I\n",
    "        new_infectious = sigma * E\n",
    "        new_recoveries = gamma * I\n",
    "        \n",
    "        S = max(S - new_exposed, 0)\n",
    "        E = max(E + new_exposed - new_infectious, 0)\n",
    "        I = max(I + new_infectious - new_recoveries, 0)\n",
    "        R = max(R + new_recoveries, 0)\n",
    "        \n",
    "        # Update the predictor buffer with the new state (use previous I as a proxy for prev_I)\n",
    "        prev_I = history['I'][-1]\n",
    "        predictor.update_buffer([current_day, S, E, I, R, prev_I])\n",
    "        \n",
    "        history['days'].append(current_day)\n",
    "        history['S'].append(S)\n",
    "        history['E'].append(E)\n",
    "        history['I'].append(I)\n",
    "        history['R'].append(R)\n",
    "        history['Beta'].append(predicted_beta)\n",
    "        \n",
    "        # Optionally stop early if the disease nearly dies out\n",
    "        if stop_early and I < 1e-7 and E < 1e-7:\n",
    "            break\n",
    "            \n",
    "    return history\n",
    "\n",
    "###############################################################################\n",
    "# 5. Training pipeline\n",
    "###############################################################################\n",
    "def train_and_save_model(data_dir, num_seeds, model_save_path, scaler_save_path, window_size=7):\n",
    "    print(f\"Training LSTM model on {num_seeds} seeds...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X, y, scaler = load_and_prepare_data(data_dir, num_seeds, window_size)\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No valid training data found.\")\n",
    "    \n",
    "    model, history = train_lstm_model(X, y, window_size)\n",
    "    \n",
    "    model.save(model_save_path)\n",
    "    joblib.dump(scaler, scaler_save_path)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f}s\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(history.history['loss'], label='Train loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "    plt.title('Training history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, scaler, training_time\n",
    "\n",
    "###############################################################################\n",
    "# 6. Prediction and evaluation\n",
    "###############################################################################\n",
    "def predict_multiple_seeds(seeds, start_day, data_dir, model_path, scaler_path, window_size):\n",
    "    \"\"\"\n",
    "    Loads the model and scaler, simulates from 'start_day' for each seed,\n",
    "    plots actual vs. predicted data, and computes RMSE for I and Beta.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    full_scaler = joblib.load(scaler_path)\n",
    "    predictor = LSTMPredictor(model, full_scaler, window_size=window_size)\n",
    "    \n",
    "    results = []\n",
    "    for seed in seeds:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_dir, f'seir_seed_{seed}.csv'))\n",
    "            df[['S', 'E', 'I', 'R', 'Beta']] = df[['S', 'E', 'I', 'R', 'Beta']].fillna(0)\n",
    "            if 'day' not in df.columns:\n",
    "                df['day'] = range(len(df))\n",
    "            df['prev_I'] = df['I'].shift(-2).fillna(0)\n",
    "            \n",
    "            history = simulate_seir_lstm(df, start_day, predictor)\n",
    "            \n",
    "            sim_length = len(history['I'])\n",
    "            end_day = min(start_day + sim_length, len(df))\n",
    "            \n",
    "            actual_I = df['I'].iloc[start_day:end_day].values\n",
    "            actual_Beta = df['Beta'].iloc[start_day:end_day].values\n",
    "            pred_I = history['I'][: (end_day - start_day)]\n",
    "            pred_Beta = history['Beta'][: (end_day - start_day)]\n",
    "            \n",
    "            rmse_I = np.sqrt(mean_squared_error(actual_I, pred_I)) if len(actual_I) > 0 else np.nan\n",
    "            rmse_Beta = np.sqrt(mean_squared_error(actual_Beta, pred_Beta)) if len(actual_Beta) > 0 else np.nan\n",
    "            \n",
    "            # Plot the results\n",
    "            fig, ax1 = plt.subplots(figsize=(12,12))\n",
    "            ax2 = ax1.twinx()\n",
    "            \n",
    "            ax1.plot(df['day'], df['I'], '--', color='orange', label='Actual I')\n",
    "            ax2.plot(df['day'], df['Beta'], ':', color='grey', label='Actual beta')\n",
    "            ax1.plot(history['days'], history['I'], '-', color='blue', label='Predicted I')\n",
    "            ax2.plot(history['days'], history['Beta'], '-.', color='green', label='Predicted beta')\n",
    "            ax1.axvline(start_day, color='magenta', linestyle='--', label='Start day')\n",
    "            \n",
    "            ax1.set_xlabel('Days')\n",
    "            ax1.set_ylabel('Infections', color='blue')\n",
    "            ax2.set_ylabel('Beta', color='green')\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax2.legend(loc='upper right')\n",
    "            \n",
    "            title_str = f\"Seed {seed} (Start: {start_day})\\nRMSE I: {rmse_I}, Beta: {rmse_Beta}\"\n",
    "            plt.title(title_str)\n",
    "            plt.show()\n",
    "            \n",
    "            results.append({\n",
    "                'seed': seed,\n",
    "                'rmse_I': rmse_I,\n",
    "                'rmse_Beta': rmse_Beta,\n",
    "                'pred_length': len(pred_I)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing seed {seed}: {str(e)}\")\n",
    "            results.append({'seed': seed, 'error': str(e)})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "###############################################################################\n",
    "# 7. Grid plotting function for multiple seeds\n",
    "###############################################################################\n",
    "def plot_results(seed_data, history, start_day, seed, ax):\n",
    "    \"\"\"\n",
    "    Plots simulation results on a given axis and returns RMSE for I and Beta.\n",
    "    \"\"\"\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(seed_data['day'], seed_data['I'], '--', color='orange', label='Actual I')\n",
    "    ax.plot(history['days'], history['I'], '-', color='blue', label='Predicted I')\n",
    "    ax.axvline(start_day, color='purple', linestyle='--', label='Start Day')\n",
    "    ax2.plot(seed_data['day'], seed_data['Beta'], ':', color='grey', label='Actual beta')\n",
    "    ax2.plot(history['days'], history['Beta'], '-.', color='green', label='Predicted beta')\n",
    "    ax.set_xlabel('Days')\n",
    "    ax.set_ylabel('Infections', color='blue')\n",
    "    ax2.set_ylabel('Beta', color='green')\n",
    "    \n",
    "    sim_length = len(history['I'])\n",
    "    end_day = min(start_day + sim_length, len(seed_data))\n",
    "    actual_I = seed_data['I'].iloc[start_day:end_day].values\n",
    "    actual_Beta = seed_data['Beta'].iloc[start_day:end_day].values\n",
    "    pred_I = history['I'][: (end_day - start_day)]\n",
    "    pred_Beta = history['Beta'][: (end_day - start_day)]\n",
    "    rmse_I = np.sqrt(mean_squared_error(actual_I, pred_I)) if len(actual_I) > 0 else np.nan\n",
    "    rmse_Beta = np.sqrt(mean_squared_error(actual_Beta, pred_Beta)) if len(actual_Beta) > 0 else np.nan\n",
    "    ax.set_title(f\"Seed {seed}\\nRMSE I: {rmse_I:.2f}, Beta: {rmse_Beta:.2f}\")\n",
    "    return rmse_I, rmse_Beta\n",
    "\n",
    "def plot_simulation_grid(seeds, start_day, data_dir, model, scaler, window_size):\n",
    "    num_windows = math.ceil(len(seeds) / 10)\n",
    "    results_list = []\n",
    "    all_rmse_I = []\n",
    "    all_rmse_Beta = []\n",
    "    \n",
    "    # Create predictor from the given model and scaler\n",
    "    predictor = LSTMPredictor(model, scaler, window_size=window_size)\n",
    "    \n",
    "    for window in range(num_windows):\n",
    "        window_seeds = seeds[window*10 : (window+1)*10]\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(12, 12))\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.5)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, seed in enumerate(window_seeds):\n",
    "            ax = axes[idx]\n",
    "            seed_result = {'seed': seed, 'start_day': start_day}\n",
    "            seed_start_time = time.time()\n",
    "            try:\n",
    "                file_path = os.path.join(data_dir, f'seir_seed_{seed}.csv')\n",
    "                seed_data = pd.read_csv(file_path)\n",
    "                seed_data[['S', 'E', 'I', 'R', 'Beta']] = seed_data[['S', 'E', 'I', 'R', 'Beta']].fillna(0)\n",
    "                seed_data['day'] = np.arange(len(seed_data))\n",
    "                seed_data['prev_I'] = seed_data['I'].shift(-2).fillna(0)\n",
    "                \n",
    "                if start_day >= len(seed_data):\n",
    "                    print(f\"Skipping seed {seed} - insufficient data\")\n",
    "                    seed_result['error'] = 'insufficient data'\n",
    "                    results_list.append(seed_result)\n",
    "                    ax.axis('off')\n",
    "                    continue\n",
    "                \n",
    "                # Simulate the SEIR model\n",
    "                history = simulate_seir_lstm(seed_data, start_day, predictor, max_days=200, stop_early=False)\n",
    "                # Compute peak infection and day\n",
    "                peak_I = max(history['I'])\n",
    "                peak_day = history['days'][np.argmax(history['I'])]\n",
    "                simulation_time = time.time() - seed_start_time\n",
    "                seed_result['peak_day'] = peak_day\n",
    "                seed_result['peak_I'] = peak_I\n",
    "                seed_result['simulation_time_seconds'] = simulation_time\n",
    "                \n",
    "                rmse_I, rmse_Beta = plot_results(seed_data, history, start_day, seed, ax)\n",
    "                seed_result['rmse_I'] = rmse_I\n",
    "                seed_result['rmse_Beta'] = rmse_Beta\n",
    "                results_list.append(seed_result)\n",
    "                \n",
    "                all_rmse_I.append(rmse_I)\n",
    "                all_rmse_Beta.append(rmse_Beta)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing seed {seed}: {str(e)}\")\n",
    "                seed_result['error'] = str(e)\n",
    "                results_list.append(seed_result)\n",
    "                ax.axis('off')\n",
    "        \n",
    "        # Turn off any unused subplots\n",
    "        for j in range(len(window_seeds), len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        # Create a custom legend\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], color='orange', linestyle='--', label='Actual Infections'),\n",
    "            Line2D([0], [0], color='blue', linestyle='-', label='Predicted Infections'),\n",
    "            Line2D([0], [0], color='purple', linestyle='--', label='Start Day'),\n",
    "            Line2D([0], [0], color='grey', linestyle=':', label='Actual Beta'),\n",
    "            Line2D([0], [0], color='green', linestyle='-.', label='Predicted Beta'),\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc='upper center', ncol=5, bbox_to_anchor=(0.5, 1.05))\n",
    "        fig.suptitle(f\"Seeds {window*10 + 1} to {(window+1)*10}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "        plt.show()\n",
    "    \n",
    "    return results_list, all_rmse_I, all_rmse_Beta\n",
    "\n",
    "###############################################################################\n",
    "# Main execution\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    DATA_DIR = '../data_to_train_on/' # BARABASI-ALBERT\n",
    "    TEST_DATA_DIR = '../data_to_predict_on/' # SEIR_SEEDS_V0\n",
    "    MODEL_PATH = 'seir_lstm_model_rmsprop.keras'\n",
    "    SCALER_PATH = 'lstm_scaler.pkl'\n",
    "    \n",
    "    WINDOW_SIZE = 14   # Must be the same for training and simulation\n",
    "    NUM_SEEDS = 1500   # Number of CSV files to train on\n",
    "    TEST_SEEDS = list(range(10))  # Seeds to test/evaluate\n",
    "    START_DAY = 50     # Simulation start day\n",
    "    \n",
    "    # 1) Train and save the model\n",
    "    '''\n",
    "    COMMENT THIS PART IF THERE IS NO NEED TO TRAIN AND SAVE THE MODEL\n",
    "    '''\n",
    "    model, scaler, _ = train_and_save_model(\n",
    "        DATA_DIR, \n",
    "        num_seeds=NUM_SEEDS, \n",
    "        model_save_path=MODEL_PATH, \n",
    "        scaler_save_path=SCALER_PATH, \n",
    "        window_size=WINDOW_SIZE\n",
    "    )\n",
    "    \n",
    "    # 2) Evaluate on test seeds, starting from START_DAY\n",
    "    '''\n",
    "    COMMENT THIS PART IF THERE IS NO NEED TO SEPARATELY PLOT EACH SEED\n",
    "    '''\n",
    "    results_df = predict_multiple_seeds(\n",
    "        seeds=TEST_SEEDS,\n",
    "        start_day=START_DAY,\n",
    "        data_dir=TEST_DATA_DIR,\n",
    "        model_path=MODEL_PATH,\n",
    "        scaler_path=SCALER_PATH,\n",
    "        window_size=WINDOW_SIZE\n",
    "    )\n",
    "    results_df.to_csv('lstm_predictions_rmsprop.csv', index=False)\n",
    "    print(results_df.describe())\n",
    "    \n",
    "    # 3) Plot simulation grid for multiple seeds\n",
    "    '''\n",
    "    COMMENT THIS PART IF THERE IS NO NEED TO PLOT 10 SEEDS PER WINDOW\n",
    "    '''\n",
    "    model = load_model(MODEL_PATH)\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    grid_results, grid_rmse_I, grid_rmse_Beta = plot_simulation_grid(\n",
    "        seeds=TEST_SEEDS,\n",
    "        start_day=START_DAY,\n",
    "        data_dir=TEST_DATA_DIR,\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        window_size=WINDOW_SIZE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
